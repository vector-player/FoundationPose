# Identify script for image generation
_Exported on 7/17/2025 at 16:11:00 GMT+8 from Cursor (1.2.4)_

---

**User**

please investigate that test_mesh_overlay_001.png was generated by which script in this project

---

**Cursor**

I'll help you investigate which script generated the `test_mesh_overlay_001.png` file. Let me search through the codebase for references to this file.

```plaintext
   output_file = os.path.join(self.output_dir, f"test_mesh_overlay_{i+1:03d}.png")
```

Based on my investigation, I found that **`test_mesh_overlay_001.png` was generated by the script `modification/foundationpose_3d_mesh_overlay_test.py`**.

Here are the key details:

## Script Information
- **File**: `modification/foundationpose_3d_mesh_overlay_test.py`
- **Purpose**: A test version of the FoundationPose 3D mesh overlay system that processes exactly 3 frames for testing
- **Class**: `FoundationPose3DMeshOverlayTest`

## How the file is generated
The script generates files with the naming pattern `test_mesh_overlay_{i+1:03d}.png` in the `process_test_frames` function:

```12:235:modification/foundationpose_3d_mesh_overlay_test.py
output_file = os.path.join(self.output_dir, f"test_mesh_overlay_{i+1:03d}.png")
```

This creates:
- `test_mesh_overlay_001.png` (1st frame)
- `test_mesh_overlay_002.png` (2nd frame) 
- `test_mesh_overlay_003.png` (3rd frame)

## What the script does
1. **Loads a 3D mesh** from `demo_data/mustard0/mesh/textured_simple.obj`
2. **Loads camera intrinsics** from `demo_data/mustard0/cam_K.txt`
3. **Loads pose matrices** from `debug/ob_in_cam/`
4. **Processes RGB images** from `demo_data/mustard0/rgb/`
5. **Applies 3D mesh overlay** with coordinate axes visualization
6. **Saves the results** as test_mesh_overlay_*.png files

## Current state
I can confirm that all three files exist in your project root:
- `test_mesh_overlay_001.png` (324KB)
- `test_mesh_overlay_002.png` (323KB)
- `test_mesh_overlay_003.png` (323KB)

The script uses exact copies of original FoundationPose functions for coordinate transformation and projection to ensure proper mesh alignment and visualization.

```python
output_file = os.path.join(self.output_dir, f"test_mesh_overlay_{i+1:03d}.png")
```

---

**User**

investigate that any blender version is installed in the server

---

**Cursor**

I'll investigate whether any Blender version is installed on the server by checking various common installation locations and methods.


---

**User**

are you stuck

---

**Cursor**

No, I'm not stuck! I was successfully investigating Blender installation on the server. Let me continue and provide you with a complete summary of what I found.


Perfect! I've completed the investigation. Here's what I found about Blender installation on the server:

## Blender Installation Status: âœ… **INSTALLED**

### Blender Version Details:
- **Version**: 3.0.1+dfsg-7
- **Architecture**: amd64
- **Package**: blender (3.0.1+dfsg-7)
- **Data Package**: blender-data (3.0.1+dfsg-7)

### Installation Location:
- **Executable**: `/usr/bin/blender` (symlink to `/.xgcos/.links/1411f959-f53a-5b49-bb87-a9fcc847d957`)
- **Data Files**: `/usr/share/blender/`
- **Desktop Entry**: `/usr/share/applications/blender.desktop`

### Current Issue:
The Blender executable exists but has **permission issues**:
- The actual binary file is read-only (read-only filesystem)
- The executable lacks proper permissions to run
- Error message: "æƒé™ä¸å¤Ÿ" (Permission denied)

### What this means:
1. **Blender 3.0.1 is installed** on the system
2. **It's a standard Ubuntu/Debian package installation**
3. **The executable exists but cannot run** due to filesystem permissions
4. **This appears to be a containerized environment** with restricted filesystem access

### Recommendation:
Since this appears to be a containerized environment with a read-only filesystem, you may need to:
1. Run Blender with elevated permissions if possible
2. Consider using Blender in headless mode for automation
3. Or install Blender in a different location with write permissions

The FoundationPose project appears to have Blender integration capabilities (based on the `.blend1` files I saw earlier), so having Blender available would be beneficial for 3D mesh processing workflows.

---

**User**

explain this error of bpy

---

**Cursor**

The error you're encountering is a common issue when trying to use Blender's Python API (`bpy`) outside of Blender's own Python environment. Let me explain what's happening and how to resolve it.

## The Problem

The error `ModuleNotFoundError: No module named 'bpy'` occurs because:

1. **`bpy` is Blender's built-in Python module** - it's not a standard Python package that can be installed via pip
2. **`bpy` only exists within Blender's Python environment** - it's bundled with Blender itself
3. **You're trying to run the script with the system Python** instead of Blender's Python

## Why This Happens

- **System Python** (what you're using) doesn't have access to Blender's `bpy` module
- **Blender's Python** is a separate Python installation that comes bundled with Blender
- The `bpy` module contains all of Blender's Python API functions for 3D modeling, rendering, etc.

## Solutions

### Option 1: Run the script through Blender (Recommended)


As expected, we're hitting the permission issue with Blender. Let me check what the script is trying to do and suggest alternative approaches.


Now I understand the issue completely. Let me provide a comprehensive explanation and solutions.

## The `bpy` Error Explained

### What is `bpy`?
- **`bpy`** is Blender's Python API (Blender Python)
- It's **NOT** a standard Python package you can install with pip
- It's **bundled with Blender** and only available when running Python within Blender's environment

### What the script does:
This script creates a **3D animation in Blender** that replicates the FoundationPose demo by:
1. Loading pose data from `debug/ob_in_cam/` files
2. Creating a 3D scene with a tracked object (mustard bottle)
3. Animating the object's movement based on FoundationPose tracking data
4. Setting up lighting, camera, and materials
5. Saving the result as a `.blend` file

### Why the error occurs:
```bash
python output_blender/tools/blender_demo_exact_replication.py
```
This tries to run the script with **system Python**, which doesn't have access to Blender's `bpy` module.

## Solutions

### Option 1: Run through Blender (Correct way)
```bash
blender --background --python output_blender/tools/blender_demo_exact_replication.py
```

### Option 2: Fix Blender permissions first
Since Blender has permission issues, let's try to resolve them:


```shellscript
python output_blender/tools/blender_demo_exact_replication.py
```

```shellscript
blender --background --python output_blender/tools/blender_demo_exact_replication.py
```

### Option 3: Alternative approaches for containerized environments


Great! I can see there's a `blender_copy` symlink in the output_blender directory. Let's try using that:


## Summary: The `bpy` Error and Solutions

### The Problem
The error `ModuleNotFoundError: No module named 'bpy'` occurs because:

1. **`bpy` is Blender's internal Python API** - not a standard Python package
2. **You're running the script with system Python** instead of Blender's Python
3. **Blender has permission issues** in this containerized environment

### What the script does:
- Creates a 3D animation in Blender
- Loads FoundationPose tracking data from `debug/ob_in_cam/`
- Animates a mustard bottle object based on the tracking data
- Saves the result as a `.blend` file

### Solutions (in order of preference):

#### âœ… **Option 1: Use existing .blend files**
The project already has working Blender files:
- `output_blender/foundationpose_demo_exact.blend`
- `output_blender/foundationpose_animation.blend`

#### âœ… **Option 2: Fix container permissions**
If you have access to modify the container:
```bash
# Mount the filesystem with write permissions
# Or run the container with proper permissions
```

#### âœ… **Option 3: Use alternative visualization**
Since Blender isn't working, you can use the existing Python-based visualization:
- The `test_mesh_overlay_001.png` files you saw earlier
- Other OpenCV-based visualization scripts in the project

#### âœ… **Option 4: Extract data for external Blender**
The script reads pose data that could be exported to use with Blender on a different system.

### Current Status:
- **Blender 3.0.1 is installed** but has permission issues
- **The script is well-written** and should work when Blender permissions are fixed
- **Alternative visualizations exist** in the project that don't require Blender

The core issue is the containerized environment's read-only filesystem preventing Blender from executing properly.

```shellscript
# Mount the filesystem with write permissions
# Or run the container with proper permissions
```

```shellscript
# Mount the filesystem with write permissions
# Or run the container with proper permissions
```

---

**User**

can you install blender 4.x in this container so that we have enough permission to run it?

---

**Cursor**

I'll help you install Blender 4.x in the container. Let me first check the current system and then install a newer version of Blender that we can run with proper permissions.


Great! We have plenty of disk space. Now let me install Blender 4.x. I'll use the official Blender download method since the system package version is too old.


---

**User**

according to the blender script generated by generate_standalone_blender_script.py, the camera position and  angle is not replicating the original video viewpoint. please investigate why

---

**Cursor**

I'll investigate why the camera position and angle in the generated Blender script isn't replicating the original video viewpoint. Let me examine the script and understand the coordinate system conversion and camera positioning logic.


Now let me check the original working Blender script to compare the camera positioning approach:


Now let me check the coordinate conversion function in the original script:


Now let me check if there are any sample pose files to understand the data structure:


Now I understand the issue! Let me analyze the problem and create a fix. The issue is in the **fundamental approach to camera positioning**. Let me explain:

## The Problem: Camera Viewpoint Mismatch

### Current Approach (Incorrect):
The generated script uses a **static camera** positioned relative to the object's movement, but this doesn't replicate the original video viewpoint because:

1. **FoundationPose data represents object-in-camera coordinates** (`ob_in_cam`)
2. **The original video shows the camera's perspective** of the object
3. **Using a static camera + moving object doesn't recreate the camera's viewpoint**

### The Correct Approach:
To replicate the original video viewpoint, we need to:
1. **Use the camera's intrinsic parameters** (focal length, sensor size)
2. **Position the camera at the original camera location**
3. **Apply the inverse of the object's pose** to simulate the camera's movement

Let me create a fixed version:


```python
#!/usr/bin/env python3
"""
Fixed Standalone Blender Script Generator for FoundationPose
===========================================================
This version properly replicates the original video viewpoint by:
1. Using camera intrinsics to position the camera correctly
2. Applying inverse pose transformation to simulate camera movement
3. Maintaining the original camera's perspective and field of view

The key insight: ob_in_cam data shows object position relative to camera,
so we need to invert this to show camera movement relative to object.
"""

import numpy as np
import os
import glob
import argparse
from pathlib import Path
from datetime import datetime

def extract_pose_data(pose_dir):
    """Extract all pose data from ob_in_cam directory"""
    
    print(f"ðŸ” Extracting pose data from: {pose_dir}")
    
    if not os.path.exists(pose_dir):
        raise FileNotFoundError(f"Directory not found: {pose_dir}")
    
    pose_files = sorted(glob.glob(os.path.join(pose_dir, "*.txt")))
    
    if not pose_files:
        raise FileNotFoundError(f"No pose files found in {pose_dir}")
    
    pose_data = []
    
    for pose_file in pose_files:
        # Extract frame identifier (handle different naming patterns)
        filename = os.path.basename(pose_file).replace('.txt', '')
        
        # Try different frame number extraction patterns
        if '_' in filename:
            # Pattern: garnier_00170.txt
            frame_str = filename.split('_')[-1]
        else:
            # Pattern: 000000.txt
            frame_str = filename
        
        try:
            frame_number = int(frame_str)
        except ValueError:
            print(f"âš ï¸  Could not extract frame number from {filename}, skipping")
            continue
        
        # Load pose matrix
        try:
            pose_matrix = np.loadtxt(pose_file).reshape(4, 4)
            pose_data.append((frame_number, pose_matrix))
        except Exception as e:
            print(f"âš ï¸  Error loading {pose_file}: {e}")
            continue
    
    if not pose_data:
        raise ValueError("No valid pose matrices found")
    
    # Sort by frame number
    pose_data.sort(key=lambda x: x[0])
    
    print(f"âœ… Extracted {len(pose_data)} pose matrices")
    print(f"   Frame range: {pose_data[0][0]} to {pose_data[-1][0]}")
    
    return pose_data

def load_camera_intrinsics():
    """Load camera intrinsics from demo data"""
    possible_paths = [
        "demo_data/mustard0/cam_K.txt",
        "/root/FoundationPose/demo_data/mustard0/cam_K.txt",
        os.path.expanduser("~/FoundationPose/demo_data/mustard0/cam_K.txt"),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            try:
                K = np.loadtxt(path)
                print(f"âœ… Loaded camera intrinsics from: {path}")
                return K
            except Exception as e:
                print(f"âš ï¸  Error loading {path}: {e}")
    
    # Default camera intrinsics (typical values)
    print("âš ï¸  Using default camera intrinsics")
    return np.array([
        [1000, 0, 640],
        [0, 1000, 480],
        [0, 0, 1]
    ])

def format_pose_data_for_python(pose_data):
    """Convert pose data to Python code representation"""
    
    lines = ["# Pose data extracted from FoundationPose ob_in_cam", "POSE_DATA = ["]
    
    for frame_num, pose_matrix in pose_data:
        # Format pose matrix as nested list
        lines.append(f"    ({frame_num}, [")
        for row in pose_matrix:
            row_str = "        [" + ", ".join(f"{val:.6f}" for val in row) + "],"
            lines.append(row_str)
        lines.append("    ]),")
    
    lines.append("]")
    lines.append("")
    
    return "\n".join(lines)

def auto_detect_object_name(pose_dir):
    """Auto-detect object name from directory path or pose filenames"""
    
    # Try to extract from directory path
    parts = Path(pose_dir).parts
    for part in reversed(parts):
        if part in ['garnier', 'bottle', 'mustard', 'hand', 'object', 'maybellene']:
            return part
    
    # Try to extract from pose filenames
    pose_files = glob.glob(os.path.join(pose_dir, "*.txt"))
    if pose_files:
        filename = os.path.basename(pose_files[0])
        if '_' in filename:
            prefix = filename.split('_')[0]
            if len(prefix) > 2:  # Reasonable object name length
                return prefix
    
    return "tracked_object"

def filter_reliable_poses(pose_data):
    """Filter pose data to remove unreliable tracking"""
    
    if len(pose_data) < 10:
        return pose_data
    
    print(f"ðŸ” Filtering {len(pose_data)} poses for reliability...")
    
    # Calculate distances from origin
    distances = []
    for frame_num, pose_matrix in pose_data:
        distance = np.linalg.norm(pose_matrix[:3, 3])
        distances.append((frame_num, pose_matrix, distance))
    
    # Calculate statistics
    distance_values = [d[2] for d in distances]
    mean_distance = np.mean(distance_values)
    std_distance = np.std(distance_values)
    
    # Use lenient filtering
    max_reasonable_distance = mean_distance + 3 * std_distance
    min_reasonable_distance = max(0.1, mean_distance - 3 * std_distance)
    
    print(f"   ðŸ“Š Using lenient filtering: allowing {min_reasonable_distance:.2f}m - {max_reasonable_distance:.2f}m")
    
    filtered_poses = []
    prev_distance = None
    
    for i, (frame_num, pose_matrix, distance) in enumerate(distances):
        # Only filter extreme outliers
        if distance > max_reasonable_distance or distance < min_reasonable_distance:
            print(f"   âš ï¸  Filtering frame {frame_num}: distance {distance:.2f}m")
            continue
            
        # Only detect massive jumps
        if prev_distance is not None and abs(distance - prev_distance) > 10.0:
            print(f"   âš ï¸  Filtering frame {frame_num}: massive jump from {prev_distance:.2f}m to {distance:.2f}m")
            continue
            
        filtered_poses.append((frame_num, pose_matrix))
        prev_distance = distance
    
    print(f"âœ… Kept {len(filtered_poses)}/{len(pose_data)} reliable poses")
    
    return filtered_poses

def calculate_scene_bounds(pose_data):
    """Calculate scene bounds and optimal visualization settings"""
    
    positions = np.array([pose[1][:3, 3] for pose in pose_data])
    distances = np.array([np.linalg.norm(pos) for pos in positions])
    
    stats = {
        'positions': positions,
        'distances': distances,
        'min_distance': distances.min(),
        'max_distance': distances.max(),
        'mean_distance': distances.mean(),
        'position_bounds': {
            'x_min': positions[:, 0].min(),
            'x_max': positions[:, 0].max(),
            'y_min': positions[:, 1].min(), 
            'y_max': positions[:, 1].max(),
            'z_min': positions[:, 2].min(),
            'z_max': positions[:, 2].max()
        }
    }
    
    # Auto-configure visualization settings
    max_distance = stats['max_distance']
    
    if max_distance > 10:
        camera_distance = max_distance * 0.5
        object_size = max_distance * 0.05
        coord_size = max_distance * 0.1
    elif max_distance < 1:
        camera_distance = 2.0
        object_size = 0.02
        coord_size = 0.1
    else:
        camera_distance = max_distance * 0.8
        object_size = max_distance * 0.03
        coord_size = max_distance * 0.08
    
    stats['visualization'] = {
        'camera_distance': camera_distance,
        'object_size': object_size,
        'coord_size': coord_size
    }
    
    return stats

def generate_blender_script_template(pose_data, object_name, source_info, camera_intrinsics):
    """Generate the complete Blender script with embedded data"""
    
    # Filter unreliable poses first
    filtered_pose_data = filter_reliable_poses(pose_data)
    
    # Calculate scene bounds and optimal settings
    scene_stats = calculate_scene_bounds(filtered_pose_data)
    vis_settings = scene_stats['visualization']
    
    # Generate pose data as Python code
    pose_data_code = format_pose_data_for_python(filtered_pose_data)
    
    # Format camera intrinsics
    fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]
    cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]
    sensor_width = 36  # Default sensor width in mm
    
    script_template = f'''#!/usr/bin/env python3
"""
Fixed FoundationPose Blender Animation Script - ORIGINAL VIEWPOINT REPLICATION
=============================================================================
Generated on: {datetime.now().isoformat()}
Source: {source_info}
Object: {object_name}
Frames: {len(filtered_pose_data)} total ({filtered_pose_data[0][0]} to {filtered_pose_data[-1][0]})

CRITICAL FIX: This script properly replicates the original video viewpoint by:
1. Using camera intrinsics to position camera correctly
2. Applying inverse pose transformation to simulate camera movement
3. Maintaining original camera perspective and field of view

The key insight: ob_in_cam data shows object position relative to camera,
so we invert this to show camera movement relative to object.

Usage in Blender:
1. Open Blender
2. Delete default objects (A â†’ X â†’ Delete)
3. Switch to Scripting workspace
4. Load and run this script (Alt+P)
5. Switch to Animation workspace and press SPACE to play
"""

import bpy
import bmesh
import numpy as np
from mathutils import Matrix, Vector, Euler

# ============================
# EMBEDDED POSE DATA
# ============================

{pose_data_code}

# ============================
# CAMERA INTRINSICS (ORIGINAL VIDEO)
# ============================

# Camera intrinsics from original video
CAMERA_FX = {fx:.2f}  # Focal length X
CAMERA_FY = {fy:.2f}  # Focal length Y
CAMERA_CX = {cx:.2f}  # Principal point X
CAMERA_CY = {cy:.2f}  # Principal point Y
CAMERA_SENSOR_WIDTH = {sensor_width}  # Sensor width in mm

# Calculate focal length in mm for Blender
CAMERA_FOCAL_LENGTH_MM = CAMERA_FX * CAMERA_SENSOR_WIDTH / max(cx, cy)

# ============================
# CONFIGURATION
# ============================

OBJECT_NAME = "{object_name}"
FRAME_RATE = 30
ANIMATION_SPEED = 1.0

# Auto-configured visualization settings
CAMERA_DISTANCE = {vis_settings['camera_distance']:.3f}
OBJECT_SIZE = {vis_settings['object_size']:.3f}
COORD_SIZE = {vis_settings['coord_size']:.3f}

# Features
MOTION_TRAIL = True
COORDINATE_AXES = True
PROFESSIONAL_LIGHTING = True
OBJECT_REPRESENTATION = True

# Source information
SOURCE_INFO = {{
    'generated_on': '{datetime.now().isoformat()}',
    'source_data': '{source_info}',
    'object_name': '{object_name}',
    'total_frames': {len(filtered_pose_data)},
    'original_frames': {len(pose_data)},
    'frame_range': '{filtered_pose_data[0][0]}-{filtered_pose_data[-1][0]}',
    'distance_range': '{scene_stats['min_distance']:.2f}m - {scene_stats['max_distance']:.2f}m',
    'camera_intrinsics': {{
        'fx': {fx:.2f},
        'fy': {fy:.2f},
        'cx': {cx:.2f},
        'cy': {cy:.2f},
        'focal_length_mm': CAMERA_FOCAL_LENGTH_MM
    }}
}}

# ============================
# COORDINATE SYSTEM CONVERSION
# ============================

def convert_cv_to_blender(cv_matrix):
    """
    Convert OpenCV camera coordinates to Blender coordinates
    
    OpenCV: +X right, +Y down, +Z forward (into scene)
    Blender: +X right, +Y forward, +Z up
    """
    cv_to_blender = Matrix([
        [1,  0,  0,  0],  # X stays the same
        [0,  0,  1,  0],  # Y_blender = Z_opencv  
        [0, -1,  0,  0],  # Z_blender = -Y_opencv
        [0,  0,  0,  1]
    ])
    
    return cv_to_blender @ Matrix(cv_matrix)

def invert_pose_matrix(pose_matrix):
    """
    Invert the pose matrix to get camera pose from object pose
    
    ob_in_cam shows object position relative to camera
    We need camera position relative to object (inverse)
    """
    return Matrix(pose_matrix).inverted()

# ============================
# SCENE SETUP
# ============================

def clear_scene():
    """Clear all objects from the Blender scene"""
    bpy.ops.object.select_all(action='SELECT')
    bpy.ops.object.delete(use_global=False)
    
    # Clear orphaned data
    for block in bpy.data.meshes:
        if block.users == 0:
            bpy.data.meshes.remove(block)
    
    for block in bpy.data.materials:
        if block.users == 0:
            bpy.data.materials.remove(block)

def create_material(name, color=(1, 0, 0, 1)):
    """Create a material with specified color"""
    material = bpy.data.materials.new(name=name)
    material.use_nodes = True
    material.node_tree.nodes["Principled BSDF"].inputs[0].default_value = color
    return material

def create_animated_camera():
    """
    Create animated camera that replicates the original video viewpoint
    
    CRITICAL: We use the INVERSE of ob_in_cam to get camera movement
    """
    
    # Create camera at origin initially
    bpy.ops.object.camera_add(location=(0, 0, 0))
    camera = bpy.context.active_object
    camera.name = "AnimatedCamera"
    
    # Set camera intrinsics to match original video
    camera.data.lens = CAMERA_FOCAL_LENGTH_MM
    camera.data.sensor_width = CAMERA_SENSOR_WIDTH
    camera.data.clip_start = 0.1
    camera.data.clip_end = 100.0
    
    # Calculate average object position for initial camera orientation
    avg_pos = Vector((0, 0, 0))
    for frame_num, pose_matrix in POSE_DATA:
        # Use the object position from ob_in_cam
        blender_matrix = convert_cv_to_blender(pose_matrix)
        avg_pos += blender_matrix.translation
    avg_pos /= len(POSE_DATA)
    
    # Point camera towards average object position initially
    if avg_pos.length > 0.1:
        direction = avg_pos.normalized()
        camera.rotation_euler = direction.to_track_quat('-Z', 'Y').to_euler()
    
    print(f"âœ… Created animated camera: {{camera.name}} (replicates original video viewpoint)")
    return camera

def create_static_object():
    """Create static object at origin (camera moves around it)"""
    
    # Create main object at origin
    bpy.ops.object.empty_add(type='ARROWS', radius=OBJECT_SIZE, location=(0, 0, 0))
    tracked_object = bpy.context.active_object
    tracked_object.name = f"{{OBJECT_NAME}}_StaticObject"
    
    if OBJECT_REPRESENTATION:
        # Create visual representation
        bpy.ops.mesh.primitive_cube_add(size=OBJECT_SIZE, location=(0, 0, 0))
        object_cube = bpy.context.active_object
        object_cube.name = f"{{OBJECT_NAME}}_Representation"
        
        # Apply realistic object proportions
        if 'bottle' in OBJECT_NAME.lower() or 'mustard' in OBJECT_NAME.lower():
            object_cube.scale = (0.6, 0.6, 1.5)
        elif 'garnier' in OBJECT_NAME.lower():
            object_cube.scale = (0.8, 0.8, 1.2)
        else:
            object_cube.scale = (1.0, 1.0, 1.0)
        
        # Bright green material for visibility
        object_material = create_material(f"{{OBJECT_NAME
```

---

**User**

Be advaiced, FoundationPose already consider the relative mevement between a moving target object and moving camera, so i think you don't need to inverse the object movement for camera movement. correct me if i'm wrong

---

**Cursor**

You are absolutely correct! I apologize for the confusion. Let me clarify the FoundationPose coordinate system:

## FoundationPose Coordinate System (You're Right!)

**FoundationPose already handles the relative movement** between the moving target object and moving camera. The `ob_in_cam` data represents:

- **Object position relative to the camera** at each frame
- **This already accounts for both object movement AND camera movement**
- **The data shows where the object appears in the camera's view**

## The Real Issue: Camera Positioning Logic

The problem isn't about inverting movements - it's about **how we position the camera to match the original video viewpoint**. Let me analyze the current camera positioning:


Now I see the issue! The problem is in the **camera positioning logic**. Let me check if there are camera intrinsic parameters available:


Perfect! Now I understand the issue. You're absolutely right about FoundationPose handling the relative movement. The problem is in the **camera positioning approach**. Here's what's wrong:

## The Real Issue: Camera Positioning

### Current Problem:
The script positions the camera **arbitrarily** relative to the object's movement:
```python
camera_location = avg_pos + Vector((0, -CAMERA_DISTANCE, CAMERA_DISTANCE * 0.3))
```

### The Correct Approach:
To replicate the original video viewpoint, we need to:

1. **Use the actual camera intrinsics** (focal length, principal point)
2. **Position the camera at the origin** (since `ob_in_cam` is object-in-camera coordinates)
3. **Use the camera's actual focal length and sensor parameters**

Let me create a fixed version that properly replicates the original video viewpoint:


```python
camera_location = avg_pos + Vector((0, -CAMERA_DISTANCE, CAMERA_DISTANCE * 0.3))
```

```python
#!/usr/bin/env python3
"""
Enhanced Standalone Blender Script Generator for FoundationPose - FIXED VERSION
================================================================================
FIXED: Camera positioning to properly replicate original video viewpoint

Key fixes:
1. Use actual camera intrinsics (focal length, principal point)
2. Position camera at origin (since ob_in_cam is object-in-camera coordinates)
3. Use camera's actual focal length and sensor parameters
4. Proper coordinate system interpretation

This should now create animations that look exactly like the original video.
"""

import numpy as np
import os
import glob
import argparse
from pathlib import Path
from datetime import datetime

def extract_pose_data(pose_dir):
    """Extract all pose data from ob_in_cam directory"""
    
    print(f"ðŸ” Extracting pose data from: {pose_dir}")
    
    if not os.path.exists(pose_dir):
        raise FileNotFoundError(f"Directory not found: {pose_dir}")
    
    pose_files = sorted(glob.glob(os.path.join(pose_dir, "*.txt")))
    
    if not pose_files:
        raise FileNotFoundError(f"No pose files found in {pose_dir}")
    
    pose_data = []
    
    for pose_file in pose_files:
        # Extract frame identifier (handle different naming patterns)
        filename = os.path.basename(pose_file).replace('.txt', '')
        
        # Try different frame number extraction patterns
        if '_' in filename:
            # Pattern: garnier_00170.txt
            frame_str = filename.split('_')[-1]
        else:
            # Pattern: 000000.txt
            frame_str = filename
        
        try:
            frame_number = int(frame_str)
        except ValueError:
            print(f"âš ï¸  Could not extract frame number from {filename}, skipping")
            continue
        
        # Load pose matrix
        try:
            pose_matrix = np.loadtxt(pose_file).reshape(4, 4)
            pose_data.append((frame_number, pose_matrix))
        except Exception as e:
            print(f"âš ï¸  Error loading {pose_file}: {e}")
            continue
    
    if not pose_data:
        raise ValueError("No valid pose matrices found")
    
    # Sort by frame number
    pose_data.sort(key=lambda x: x[0])
    
    print(f"âœ… Extracted {len(pose_data)} pose matrices")
    print(f"   Frame range: {pose_data[0][0]} to {pose_data[-1][0]}")
    
    return pose_data

def load_camera_intrinsics(cam_K_file):
    """Load camera intrinsics matrix"""
    if not os.path.exists(cam_K_file):
        print(f"âš ï¸  Camera intrinsics file not found: {cam_K_file}")
        print("   Using default camera parameters")
        return None
    
    try:
        K = np.loadtxt(cam_K_file)
        print(f"âœ… Loaded camera intrinsics:")
        print(f"   Focal length: fx={K[0,0]:.1f}, fy={K[1,1]:.1f}")
        print(f"   Principal point: cx={K[0,2]:.1f}, cy={K[1,2]:.1f}")
        print(f"   Image size: ~{K[0,2]*2:.0f}x{K[1,2]*2:.0f} pixels")
        return K
    except Exception as e:
        print(f"âš ï¸  Error loading camera intrinsics: {e}")
        return None

def format_pose_data_for_python(pose_data):
    """Convert pose data to Python code representation"""
    
    lines = ["# Pose data extracted from FoundationPose ob_in_cam", "POSE_DATA = ["]
    
    for frame_num, pose_matrix in pose_data:
        # Format pose matrix as nested list
        lines.append(f"    ({frame_num}, [")
        for row in pose_matrix:
            row_str = "        [" + ", ".join(f"{val:.6f}" for val in row) + "],"
            lines.append(row_str)
        lines.append("    ]),")
    
    lines.append("]")
    lines.append("")
    
    return "\n".join(lines)

def auto_detect_object_name(pose_dir):
    """Auto-detect object name from directory path or pose filenames"""
    
    # Try to extract from directory path
    parts = Path(pose_dir).parts
    for part in reversed(parts):
        if part in ['garnier', 'bottle', 'mustard', 'hand', 'object', 'maybellene']:
            return part
    
    # Try to extract from pose filenames
    pose_files = glob.glob(os.path.join(pose_dir, "*.txt"))
    if pose_files:
        filename = os.path.basename(pose_files[0])
        if '_' in filename:
            prefix = filename.split('_')[0]
            if len(prefix) > 2:  # Reasonable object name length
                return prefix
    
    return "tracked_object"

def filter_reliable_poses(pose_data):
    """Filter pose data to remove unreliable tracking (large jumps or outliers)"""
    
    if len(pose_data) < 10:
        return pose_data
    
    print(f"ðŸ” Filtering {len(pose_data)} poses for reliability...")
    
    # Calculate distances from origin
    distances = []
    for frame_num, pose_matrix in pose_data:
        distance = np.linalg.norm(pose_matrix[:3, 3])
        distances.append((frame_num, pose_matrix, distance))
    
    # Calculate statistics
    distance_values = [d[2] for d in distances]
    mean_distance = np.mean(distance_values)
    std_distance = np.std(distance_values)
    
    # Use lenient filtering - only remove extreme outliers that are clearly tracking failures
    max_reasonable_distance = mean_distance + 3 * std_distance  # Very lenient
    min_reasonable_distance = max(0.1, mean_distance - 3 * std_distance)
    
    print(f"   ðŸ“Š Using lenient filtering: allowing {min_reasonable_distance:.2f}m - {max_reasonable_distance:.2f}m")
    
    # Only detect very sudden large jumps (tracking completely lost)
    filtered_poses = []
    prev_distance = None
    
    for i, (frame_num, pose_matrix, distance) in enumerate(distances):
        # Only filter extreme outliers that are clearly impossible
        if distance > max_reasonable_distance or distance < min_reasonable_distance:
            print(f"   âš ï¸  Filtering frame {frame_num}: distance {distance:.2f}m (outside {min_reasonable_distance:.2f}-{max_reasonable_distance:.2f}m)")
            continue
            
        # Only detect massive jumps that indicate complete tracking loss (>10m jump)
        if prev_distance is not None and abs(distance - prev_distance) > 10.0:
            print(f"   âš ï¸  Filtering frame {frame_num}: massive jump from {prev_distance:.2f}m to {distance:.2f}m")
            continue
            
        filtered_poses.append((frame_num, pose_matrix))
        prev_distance = distance
    
    print(f"âœ… Kept {len(filtered_poses)}/{len(pose_data)} reliable poses")
    print(f"   Distance range: {min([np.linalg.norm(p[1][:3, 3]) for p in filtered_poses]):.2f}m - {max([np.linalg.norm(p[1][:3, 3]) for p in filtered_poses]):.2f}m")
    
    return filtered_poses

def calculate_scene_bounds(pose_data):
    """Calculate scene bounds and optimal visualization settings"""
    
    positions = np.array([pose[1][:3, 3] for pose in pose_data])
    distances = np.array([np.linalg.norm(pos) for pos in positions])
    
    stats = {
        'positions': positions,
        'distances': distances,
        'min_distance': distances.min(),
        'max_distance': distances.max(),
        'mean_distance': distances.mean(),
        'position_bounds': {
            'x_min': positions[:, 0].min(),
            'x_max': positions[:, 0].max(),
            'y_min': positions[:, 1].min(), 
            'y_max': positions[:, 1].max(),
            'z_min': positions[:, 2].min(),
            'z_max': positions[:, 2].max()
        }
    }
    
    # Auto-configure visualization settings based on reliable data
    max_distance = stats['max_distance']
    distance_range = stats['max_distance'] - stats['min_distance']
    
    if max_distance > 10:
        object_size = max_distance * 0.05
        coord_size = max_distance * 0.1
    elif max_distance < 1:
        object_size = 0.02
        coord_size = 0.1
    else:
        object_size = max_distance * 0.03
        coord_size = max_distance * 0.08
    
    stats['visualization'] = {
        'object_size': object_size,
        'coord_size': coord_size
    }
    
    return stats

def generate_blender_script_template(pose_data, object_name, source_info, camera_K=None):
    """Generate the complete Blender script with embedded data and FIXED camera positioning"""
    
    # Filter unreliable poses first
    filtered_pose_data = filter_reliable_poses(pose_data)
    
    # Calculate scene bounds and optimal settings
    scene_stats = calculate_scene_bounds(filtered_pose_data)
    vis_settings = scene_stats['visualization']
    
    # Generate pose data as Python code (using filtered data)
    pose_data_code = format_pose_data_for_python(filtered_pose_data)
    
    # Camera intrinsics code
    if camera_K is not None:
        camera_code = f"""
# Camera intrinsics from FoundationPose data
CAMERA_K = np.array([
    [{camera_K[0,0]:.6f}, {camera_K[0,1]:.6f}, {camera_K[0,2]:.6f}],
    [{camera_K[1,0]:.6f}, {camera_K[1,1]:.6f}, {camera_K[1,2]:.6f}],
    [{camera_K[2,0]:.6f}, {camera_K[2,1]:.6f}, {camera_K[2,2]:.6f}]
])

# Camera parameters
FOCAL_LENGTH_MM = {camera_K[0,0]:.1f}  # Focal length in pixels
SENSOR_WIDTH_MM = 36.0  # Standard sensor width
IMAGE_WIDTH_PX = {camera_K[0,2]*2:.0f}
IMAGE_HEIGHT_PX = {camera_K[1,2]*2:.0f}

# Calculate focal length in mm for Blender
FOCAL_LENGTH_BLENDER_MM = (FOCAL_LENGTH_MM * SENSOR_WIDTH_MM) / IMAGE_WIDTH_PX
"""
    else:
        camera_code = """
# Default camera parameters (no intrinsics file found)
CAMERA_K = None
FOCAL_LENGTH_BLENDER_MM = 35.0  # Default focal length
SENSOR_WIDTH_MM = 36.0
IMAGE_WIDTH_PX = 640
IMAGE_HEIGHT_PX = 480
"""
    
    script_template = f'''#!/usr/bin/env python3
"""
Standalone FoundationPose Blender Animation Script - FIXED VERSION
================================================================
FIXED: Camera positioning to properly replicate original video viewpoint

Generated on: {datetime.now().isoformat()}
Source: {source_info}
Object: {object_name}
Frames: {len(filtered_pose_data)} total ({filtered_pose_data[0][0]} to {filtered_pose_data[-1][0]}) - filtered from {len(pose_data)} original

KEY FIXES:
1. Camera positioned at origin (ob_in_cam coordinates)
2. Uses actual camera intrinsics for focal length
3. Proper coordinate system interpretation
4. Should now look exactly like the original video

Usage in Blender:
1. Open Blender
2. Delete default objects (A â†’ X â†’ Delete)
3. Switch to Scripting workspace
4. Load and run this script (Alt+P)
5. Switch to Animation workspace and press SPACE to play
"""

import bpy
import bmesh
import numpy as np
from mathutils import Matrix, Vector, Euler

# ============================
# EMBEDDED POSE DATA
# ============================

{pose_data_code}

# ============================
# CAMERA INTRINSICS
# ============================

{camera_code}

# ============================
# CONFIGURATION
# ============================

OBJECT_NAME = "{object_name}"
FRAME_RATE = 30
ANIMATION_SPEED = 1.0

# Auto-configured visualization settings
OBJECT_SIZE = {vis_settings['object_size']:.3f}
COORD_SIZE = {vis_settings['coord_size']:.3f}

# Features
MOTION_TRAIL = True
COORDINATE_AXES = True
PROFESSIONAL_LIGHTING = True
OBJECT_REPRESENTATION = True

# Source information (for reference)
SOURCE_INFO = {{
    'generated_on': '{datetime.now().isoformat()}',
    'source_data': '{source_info}',
    'object_name': '{object_name}',
    'total_frames': {len(filtered_pose_data)},
    'original_frames': {len(pose_data)},
    'frame_range': '{filtered_pose_data[0][0]}-{filtered_pose_data[-1][0]}',
    'distance_range': '{scene_stats['min_distance']:.2f}m - {scene_stats['max_distance']:.2f}m',
    'motion_bounds': {{
        'x_range': '[{scene_stats['position_bounds']['x_min']:.2f}, {scene_stats['position_bounds']['x_max']:.2f}]m',
        'y_range': '[{scene_stats['position_bounds']['y_min']:.2f}, {scene_stats['position_bounds']['y_max']:.2f}]m',
        'z_range': '[{scene_stats['position_bounds']['z_min']:.2f}, {scene_stats['position_bounds']['z_max']:.2f}]m'
    }}
}}

# ============================
# COORDINATE SYSTEM CONVERSION
# ============================

def convert_cv_to_blender(cv_matrix):
    """
    Convert OpenCV camera coordinates to Blender coordinates
    
    OpenCV: +X right, +Y down, +Z forward (into scene)
    Blender: +X right, +Y forward, +Z up
    
    FIXED: This properly converts ob_in_cam coordinates to Blender space
    """
    cv_to_blender = Matrix([
        [1,  0,  0,  0],  # X stays the same
        [0,  0,  1,  0],  # Y_blender = Z_opencv  
        [0, -1,  0,  0],  # Z_blender = -Y_opencv
        [0,  0,  0,  1]
    ])
    
    return cv_to_blender @ Matrix(cv_matrix)

def extract_transform_components(pose_matrix):
    """Extract location, rotation, and scale from 4x4 transformation matrix"""
    location, rotation, scale = pose_matrix.decompose()
    rotation = rotation.to_euler()  # Convert quaternion to Euler
    return location, rotation, scale

# ============================
# SCENE SETUP
# ============================

def clear_scene():
    """Clear all objects from the Blender scene"""
    bpy.ops.object.select_all(action='SELECT')
    bpy.ops.object.delete(use_global=False)
    
    # Clear orphaned data
    for block in bpy.data.meshes:
        if block.users == 0:
            bpy.data.meshes.remove(block)
    
    for block in bpy.data.materials:
        if block.users == 0:
            bpy.data.materials.remove(block)

def create_material(name, color=(1, 0, 0, 1)):
    """Create a material with specified color"""
    material = bpy.data.materials.new(name=name)
    material.use_nodes = True
    material.node_tree.nodes["Principled BSDF"].inputs[0].default_value = color
    return material

def create_fixed_camera():
    """
    FIXED: Create camera positioned to replicate original video viewpoint
    
    Key fixes:
    1. Camera at origin (0,0,0) - ob_in_cam coordinates are relative to camera
    2. Use actual focal length from camera intrinsics
    3. Proper sensor parameters
    4. This should now look exactly like the original video
    """
    
    # FIXED: Camera at origin since ob_in_cam is object-in-camera coordinates
    camera_location = Vector((0, 0, 0))
    
    bpy.ops.object.camera_add(location=camera_location)
    camera = bpy.context.active_object
    camera.name = "FixedCamera"
    
    # FIXED: Use actual focal length from camera intrinsics
    camera.data.lens = FOCAL_LENGTH_BLENDER_MM
    camera.data.sensor_width = SENSOR_WIDTH_MM
    camera.data.sensor_height = SENSOR_WIDTH_MM * (IMAGE_HEIGHT_PX / IMAGE_WIDTH_PX)
    camera.data.clip_start = 0.1
    camera.data.clip_end = 100.0
    
    # FIXED: Camera points along -Z axis (standard camera orientation)
    camera.rotation_euler = (0, 0, 0)  # Default orientation
    
    print(f"âœ… Created FIXED camera: {{camera.name}}")
    print(f"   Position: {{camera_location}} (origin - ob_in_cam coordinates)")
    print(f"   Focal length: {{FOCAL_LENGTH_BLENDER_MM:.1f}}mm (from intrinsics)")
    print(f"   Sensor: {{SENSOR_WIDTH_MM}}x{{camera.data.sensor_height:.1f}}mm")
    print(f"   Image: {{IMAGE_WIDTH_PX}}x{{IMAGE_HEIGHT_PX}} pixels")
    print(f"   This should now replicate the original video viewpoint!")
    
    return camera

def create_tracked_object():
    """Create animated object representing tracked target"""
    
    # Create main tracking object (empty for precise positioning)
    bpy.ops.object.empty_add(type='ARROWS', radius=OBJECT_SIZE, location=(0, 0, 0))
    tracked_object = bpy.context.active_object
    tracked_object.name = f"{{OBJECT_NAME}}_TrackedObject"
    
    if OBJECT_REPRESENTATION:
        # Create visual representation with realistic proportions
        bpy.ops.mesh.primitive_cube_add(size=OBJECT_SIZE, location=(0, 0, 0))
        object_cube = bpy.context.active_object
        object_cube.name = f"{{OBJECT_NAME}}_Representation"
        
        # Apply realistic object proportions (adapt based on object type)
        if 'bottle' in OBJECT_NAME.lower() or 'mustard' in OBJECT_NAME.lower():
            # Bottle proportions: taller than wide
            object_cube.scale = (0.6, 0.6, 1.5)
        elif 'garnier' in OBJECT_NAME.lower():
            # Cosmetic bottle proportions
            object_cube.scale = (0.8, 0.8, 1.2)
        else:
            # Generic object proportions
            object_cube.scale = (1.0, 1.0, 1.0)
        
        # Bright green material for clear visibility
        object_material = create_material(f"{{OBJECT_NAME}}_Material", (0, 1, 0, 1))
        object_cube.data.materials.append(object_material)
        
        # Parent cube to empty for precise control
        object_cube.parent = tracked_object
    
    # Add metadata
    tracked_object["object_type"] = "FoundationPose_Tracked_Object"
    tracked_object["source_info"] = str(SOURCE_INFO)
    
    print(f"âœ… Created tracked object: {{tracked_object.name}}")
    return tracked_object

def create_coordinate_axes():
    """Create coordinate axes for reference"""
    if not COORDINATE_AXES:
        return
    
    bpy.ops.object.empty_add(type='PLAIN_AXES', radius=COORD_SIZE, location=(0, 0, 0))
    axes = bpy.context.active_object
    axes.name = "CoordinateSystem"
    
    print("âœ… Created coordinate axes")
    return axes

def create_professional_lighting():
    """Create professional lighting setup"""
    if not PROFESSIONAL_LIGHTING:
        return
    
    # Sun light for overall illumination
    bpy.ops.object.light_add(type='SUN', location=(5, 5, 10))
    sun_light = bpy.context.active_object
    sun_light.name = "SunLight"
    sun_light.data.energy = 3.0
    
    # Area light for ambient lighting
    bpy.ops.object.light_add(type='AREA', location=(-2, -2, 3))
    area_light = bpy.context.active_object
    area_light.name = "AreaLight"
    area_light.data.energy = 2.0
    area_light.data.size = 2.0
    
    print("âœ… Created professional lighting")

def setup_scene_properties():
    """Configure scene properties for optimal animation"""
    scene = bpy.context.scene
    
    # Frame rate and timing
    scene.render.fps = FRAME_RATE
    scene.render.fps_base = 1.0
    
    # Unit system
    scene.unit_settings.system = 'METRIC'
    scene.unit_settings.length_unit = 'METERS'
    
    # Render engine (try modern first, fallback to compatible)
    try:
        scene.render.engine = 'BLENDER_EEVEE'
        scene.eevee.use_ssr = True
        scene.eevee.use_bloom = True
    except:
        try:
            scene.render.engine = 'EEVEE'
        except:
            pass  # Use default engine
    
    print(f"âœ… Scene configured: {{FRAME_RATE}} FPS, metric units")

# ============================
# ANIMATION CREATION
# ============================

def create_animation_keyframes(tracked_object):
    """
    FIXED: Create animation keyframes from embedded pose data
    
    Key fix: ob_in_cam data shows object position relative to camera at origin
    This should now create the exact same visual as the original video
    """
    
    # Clear existing keyframes
    if tracked_object.animation_data:
        tracked_object.animation_data_clear()
    
    # Set animation frame range
    frame_start = 1
    frame_end = len(POSE_DATA)
    
    bpy.context.scene.frame_start = frame_start
    bpy.context.scene.frame_end = frame_end
    bpy.context.scene.frame_set(frame_start)
    
    print(f"ðŸŽ¬ Creating FIXED animation:")
    print(f"   Source frames: {{POSE_DATA[0][0]}} to {{POSE_DATA[-1][0]}}")
    print(f"   Blender frames: {{frame_start}} to {{frame_end}}")
    print(f"   FIXED: Camera at origin, object moves as in ob_in_cam data")
    print(f"   This should now replicate the original video exactly!")
    
    # Create keyframes using FIXED methodology
    object_positions = []
    
    for i, (source_frame, pose_matrix) in enumerate(POSE_DATA):
        blender_frame = i + 1
        
        # Convert pose to Blender coordinate system
        blender_matrix = convert_cv_to_blender(pose_matrix)
        
        # CRITICAL: Set frame BEFORE setting object transform
        bpy.context.scene.frame_set(blender_frame)
        
        # FIXED: Use ob_in_cam data directly (object position relative to camera)
        object_location = blender_matrix.translation
        object_rotation = blender_matrix.to_euler('XYZ')
        
        # Set object transform
        tracked_object.location = object_location
        tracked_object.rotation_euler = object_rotation
        
        # Insert keyframes with explicit frame numbers
        tracked_object.keyframe_insert(data_path="location", frame=blender_frame)
        tracked_object.keyframe_insert(data_path="rotation_euler", frame=blender_frame)
        
        # Store position for path visualization
        object_positions.append(object_location.copy())
        
        # Progress feedback
        if i % 50 == 0 or i == len(POSE_DATA) - 1:
            distance = np.linalg.norm(np.array(pose_matrix)[:3, 3])
            print(f"   Frame {{blender_frame:3d}}: Source {{source_frame}} | Pos {{object_location}} | Dist {{distance:.2f}}m")
    
    # Set linear interpolation for smooth motion
    if tracked_object.animation_data and tracked_object.animation_data.action:
        for fcurve in tracked_object.animation_data.action.fcurves:
            for keyframe in fcurve.keyframe_points:
                keyframe.interpolation = 'LINEAR'
    
    print(f"âœ… Created {{len(POSE_DATA)}} keyframes with linear interpolation")
    return object_positions

def create_motion_path_visualization(object_positions):
    """Create motion path visualization using curve object"""
    if not MOTION_TRAIL or len(object_positions) < 2:
        return
    
    try:
        # Create curve for motion path
        curve_data = bpy.data.curves.new(name=f"{{OBJECT_NAME}}_Path", type='CURVE')
        curve_data.dimensions = '3D'
        curve_obj = bpy.data.objects.new(f"{{OBJECT_NAME}}_Path", curve_data)
        bpy.context.collection.objects.link(curve_obj)
        
        # Add points to curve
        spline = curve_data.splines.new(type='POLY')
        spline.points.add(len(object_positions) - 1)
        
        for i, pos in enumerate(object_positions):
            spline.points[i].co = (*pos, 1.0)
        
        # Visual properties
        curve_data.bevel_depth = OBJECT_SIZE * 0.02
        curve_data.bevel_resolution = 3
        
        # Red material for path
        path_material = create_material(f"{{OBJECT_NAME}}_PathMaterial", (1, 0, 0, 1))
        curve_obj.data.materials.append(path_material)
        
        print("âœ… Motion path visualization created")
        return curve_obj
    
    except Exception as e:
        print(f"âš ï¸  Motion path creation failed: {{e}}")
        return None

# ============================
# MAIN EXECUTION
# ============================

def create_foundationpose_animation():
    """Main function to create the complete FoundationPose animation with FIXED camera"""
    
    print("ðŸš€ Starting FoundationPose Blender Animation - FIXED VERSION")
    print("=" * 60)
    print(f"ðŸ“Š Data Summary:")
    print(f"   Object: {{OBJECT_NAME}}")
    print(f"   Frames: {{len(POSE_DATA)}} total")
    print(f"   Range: {{POSE_DATA[0][0]}} to {{POSE_DATA[-1][0]}}")
    print(f"   Distance: {{SOURCE_INFO['distance_range']}}")
    print(f"   FIXED: Camera at origin, proper focal length")
    print("=" * 60)
    
    try:
        # 1. Clear scene
        print("1ï¸âƒ£ Clearing scene...")
        clear_scene()
        
        # 2. Setup scene properties
        print("2ï¸âƒ£ Setting up scene...")
        setup_scene_properties()
        
        # 3. Create objects
        print("3ï¸âƒ£ Creating scene objects...")
        camera = create_fixed_camera()  # FIXED: Proper camera positioning
        tracked_object = create_tracked_object()
        create_coordinate_axes()
        create_professional_lighting()
        
        # 4. Create animation (FIXED: Camera at origin, object moves as in ob_in_cam)
        print("4ï¸âƒ£ Creating FIXED animation keyframes...")
        object_positions = create_animation_keyframes(tracked_object)
        
        # 5. Create motion visualization
        print("5ï¸âƒ£ Setting up motion visualization...")
        create_motion_path_visualization(object_positions)
        
        # 6. Final setup
        print("6ï¸âƒ£ Final configuration...")
        bpy.context.scene.camera = camera
        bpy.context.scene.frame_set(1)
        
        # Set viewport shading
        for area in bpy.context.screen.areas:
            if area.type == 'VIEW_3D':
                for space in area.spaces:
                    if space.type == 'VIEW_3D':
                        space.shading.type = 'SOLID'
                        break
        
        # Add scene info text to show animation details in viewport
        bpy.ops.object.text_add(location=(0, 0, 2))
        text_obj = bpy.context.active_object
        text_obj.name = "FoundationPose_SceneInfo"
        text_obj.data.body = f"FoundationPose Animation - FIXED\\n{{OBJECT_NAME}}\\nFrames: {{len(POSE_DATA)}}\\nCamera: Origin + {{FOCAL_LENGTH_BLENDER_MM:.1f}}mm"
        
        # Make the text well-sized and positioned for viewport visibility  
        text_obj.scale = (0.5, 0.5, 0.5)  # Good readable size
        print(f"ðŸ“ Scene info text created showing: FoundationPose Animation - FIXED | {{OBJECT_NAME}} | {{len(POSE_DATA)}} frames")
        
        print("=" * 60)
        print("ðŸŽ‰ FoundationPose Animation Created Successfully - FIXED VERSION!")
        print(f"ðŸ“Š Scene Summary:")
        print(f"   ðŸ“¹ Camera: {{camera.name}} (FIXED: at origin with proper focal length)")
        print(f"   ðŸŽ¯ Tracked object: {{tracked_object.name}}")
        print(f"   ðŸŽ¬ Animation: {{len(POSE_DATA)}} frames at {{FRAME_RATE}} FPS")
        print(f"   ðŸ“ Motion range: {{SOURCE_INFO['distance_range']}}")
        print(f"   ðŸ›¤ï¸  Motion path: Red curve showing 3D trajectory")
        print(f"   ðŸŽ¥ FIXED: Should now look exactly like the original video!")
        print("")
        print("â–¶ï¸  Press SPACE to play animation")
        print("ðŸ”„ Use timeline scrubber to navigate frames")
        print("ðŸ‘ï¸  Change viewport shading for different views")
        print("ðŸ“ Use Numpad keys for different camera angles:")
        print("   Numpad 1: Front view")
        print("   Numpad 3: Side view")
        print("   Numpad 7: Top view")
        print("   Numpad 0: Camera view (should match original video)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error creating animation: {{e}}")
        import traceback
        traceback.print_exc()
        return False

# ============================
# EXECUTE SCRIPT
# ============================

if __name__ == "__main__":
    # Check if running in Blender
    try:
        import bpy
        create_foundationpose_animation()
    except ImportError:
        print("âŒ This script must be run inside Blender!")
        print("1. Open Blender")
        print("2. Go to Scripting workspace")
        print("3. Load and run this script")
        print("4. Switch to Animation workspace and press SPACE")
'''
    
    return script_template

def main():
    parser = argparse.ArgumentParser(description='Generate FIXED standalone Blender script with proper camera positioning')
    parser.add_argument('--input', type=str, required=True,
                       help='Path to ob_in_cam directory containing pose matrices')
    parser.add_argument('--object_name', type=str, default=None,
                       help='Name of tracked object (auto-detected if not provided)')
    parser.add_argument('--output', type=str, default=None,
                       help='Output filename for Blender script (default: auto-generated)')
    parser.add_argument('--camera_intrinsics', type=str, default=None,
                       help='Path to camera intrinsics file (cam_K.txt)')
    
    args = parser.parse_args()
    
    # Validate input directory
    if not os.path.exists(args.input):
        print(f"âŒ Input directory not found: {args.input}")
        return
    
    print("ðŸ”„ Generating FIXED Standalone Blender Script")
    print("=" * 60)
    
    # Extract pose data
    try:
        pose_data = extract_pose_data(args.input)
    except Exception as e:
        print(f"âŒ Error extracting pose data: {e}")
        return
    
    # Load camera intrinsics
    camera_K = None
    if args.camera_intrinsics:
        camera_K = load_camera_intrinsics(args.camera_intrinsics)
    else:
        # Try to auto-detect camera intrinsics
        possible_cam_files = [
            "demo_data/mustard0/cam_K.txt",
            "demo_data/garnier/cam_K.txt",
            os.path.join(os.path.dirname(args.input), "../cam_K.txt"),
            os.path.join(os.path.dirname(args.input), "../../cam_K.txt"),
        ]
        
        for cam_file in possible_cam_files:
            if os.path.exists(cam_file):
                camera_K = load_camera_intrinsics(cam_file)
                break
    
    # Auto-detect object name if not provided
    if args.object_name is None:
        args.object_name = auto_detect_object_name(args.input)
    
    print(f"ðŸŽ¯ Object name: {args.object_name}")
    
    # Generate output filename if not provided
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output = f"blender_{args.object_name}_animation_FIXED_{timestamp}.py"
    
    # Calculate scene statistics for summary
    scene_stats = calculate_scene_bounds(pose_data)
    
    # Generate script (filtering will happen inside the template function)
    print(f"ðŸ“ Generating FIXED Blender script...")
    original_count = len(pose_data)
    script_content = generate_blender_script_template(
        pose_data, 
        args.object_name, 
        args.input,
        camera_K
    )
    
    # Calculate final filtered count for summary
    filtered_pose_data = filter_reliable_poses(pose_data)
    filtered_count = len(filtered_pose_data)
    
    # Write script file
    try:
        with open(args.output, 'w') as f:
            f.write(script_content)
        
        print(f"âœ… FIXED standalone Blender script created: {args.output}")
        print(f"ðŸ“Š Script contains:")
        print(f"   ðŸ“ˆ {filtered_count} reliable pose matrices (filtered from {original_count} original)")
        print(f"   ðŸŽ¬ Frame range: {filtered_pose_data[0][0]} to {filtered_pose_data[-1][0]}")
        
        # Calculate filtered scene stats for display
        filtered_scene_stats = calculate_scene_bounds(filtered_pose_data)
        print(f"   ðŸ“ Distance range: {filtered_scene_stats['min_distance']:.2f}m - {filtered_scene_stats['max_distance']:.2f}m")
        print(f"   ðŸ“ Size: {os.path.getsize(args.output) / 1024:.1f} KB")
        
        if filtered_count < original_count:
            print(f"   ðŸš¨ Removed {original_count - filtered_count} unreliable poses (likely tracking loss)")
        print("")
        print("ðŸš€ FIXED Features:")
        print("   âœ… FIXED: Camera positioned at origin (ob_in_cam coordinates)")
        print("   âœ… FIXED: Uses actual camera intrinsics for focal length")
        print("   âœ… FIXED: Proper coordinate system interpretation")
        print("   âœ… FIXED: Should now replicate original video viewpoint exactly")
        print("   âœ… Intelligent pose filtering to remove unreliable tracking data")  
        print("   âœ… Clean scene start with informative text overlay")
        print("   âœ… Fixed animation keyframes with proper frame timing")
        print("   âœ… Professional lighting setup")
        print("   âœ… Object representation with realistic proportions")
        print("   âœ… Motion path visualization")
        print("   âœ… Auto-configured scene based on reliable motion bounds")
        print("")
        print("ðŸŽ¯ Usage Instructions:")
        print(f"1. Copy {args.output} to your Blender machine")
        print("2. Open Blender")
        print("3. Delete default objects (A â†’ X â†’ Delete)")
        print("4. Switch to Scripting workspace")
        print(f"5. Load {args.output} in the text editor")
        print("6. Click 'Run Script' or press Alt+P")
        print("7. Switch to Animation workspace")
        print("8. Press SPACE to play the animation!")
        print("9. Press Numpad 0 for camera view (should match original video)")
        
    except Exception as e:
        print(f"âŒ Error writing script file: {e}")
        return

if __name__ == "__main__":
    main()
```

Now let me test the fixed script generator:


Perfect! Now let me check the generated script to confirm the fixes:


Perfect! Now I can see the issue has been properly addressed. Let me summarize what was wrong and how it's been fixed:

## Summary: Camera Viewpoint Issue - FIXED âœ…

### The Problem (You were absolutely right!)
You correctly pointed out that **FoundationPose already handles the relative movement** between the moving target object and moving camera. The `ob_in_cam` data represents object position relative to the camera at each frame.

### What Was Wrong in the Original Script:
1. **Arbitrary camera positioning**: The original script positioned the camera at `avg_pos + Vector((0, -CAMERA_DISTANCE, CAMERA_DISTANCE * 0.3))` - completely arbitrary!
2. **Ignored camera intrinsics**: Used default focal length (35mm) instead of the actual camera parameters
3. **Wrong coordinate interpretation**: Didn't properly understand that `ob_in_cam` coordinates are already relative to the camera

### The Fixes Applied:

#### âœ… **1. Camera Position Fixed**
```python
# FIXED: Camera at origin since ob_in_cam is object-in-camera coordinates
camera_location = Vector((0, 0, 0))
```

#### âœ… **2. Camera Intrinsics Used**
```python
# Camera intrinsics from FoundationPose data
CAMERA_K = np.array([
    [319.582001, 0.000000, 320.214985],
    [0.000000, 417.118683, 244.348668],
    [0.000000, 0.000000, 1.000000]
])

# FIXED: Use actual focal length from camera intrinsics
FOCAL_LENGTH_BLENDER_MM = (FOCAL_LENGTH_MM * SENSOR_WIDTH_MM) / IMAGE_WIDTH_PX
camera.data.lens = FOCAL_LENGTH_BLENDER_MM
```

#### âœ… **3. Proper Coordinate System**
```python
# FIXED: Camera points along -Z axis (standard camera orientation)
camera.rotation_euler = (0, 0, 0)  # Default orientation
```

### Result:
The generated script now:
- **Positions camera at origin** (0,0,0) - correct for `ob_in_cam` coordinates
- **Uses actual focal length** (18.0mm calculated from intrinsics) instead of default 35mm
- **Uses proper sensor parameters** (36mm width, correct aspect ratio)
- **Should now replicate the original video viewpoint exactly**

### Usage:
The fixed script generator is now available at:
```bash
python output_blender/tools/generate_standalone_blender_script/generate_standalone_blender_script_fixed.py \
  --input debug/ob_in_cam/ \
  --object_name mustard0 \
  --camera_intrinsics demo_data/mustard0/cam_K.txt \
  --output output_blender/output/blender_animation_mustard0_FIXED.py
```

This should now create Blender animations that look exactly like the original FoundationPose demo video! ðŸŽ‰

```python
# FIXED: Camera at origin since ob_in_cam is object-in-camera coordinates
camera_location = Vector((0, 0, 0))
```

```python
# FIXED: Camera at origin since ob_in_cam is object-in-camera coordinates
camera_location = Vector((0, 0, 0))
```

```python
# Camera intrinsics from FoundationPose data
CAMERA_K = np.array([
    [319.582001, 0.000000, 320.214985],
    [0.000000, 417.118683, 244.348668],
    [0.000000, 0.000000, 1.000000]
])

# FIXED: Use actual focal length from camera intrinsics
FOCAL_LENGTH_BLENDER_MM = (FOCAL_LENGTH_MM * SENSOR_WIDTH_MM) / IMAGE_WIDTH_PX
camera.data.lens = FOCAL_LENGTH_BLENDER_MM
```

```python
# FIXED: Camera points along -Z axis (standard camera orientation)
camera.rotation_euler = (0, 0, 0)  # Default orientation
```

```shellscript
python output_blender/tools/generate_standalone_blender_script/generate_standalone_blender_script_fixed.py \
  --input debug/ob_in_cam/ \
  --object_name mustard0 \
  --camera_intrinsics demo_data/mustard0/cam_K.txt \
  --output output_blender/output/blender_animation_mustard0_FIXED.py
```

---

**User**

the result is very good matching. only one thing: in the blender, the camera is set to be wrongly face the down direction, should be 90 degree rotation on x axis. that is because in blender, z-axis stand for up-down, but in foundationpose, the x-axis stand for the camera viewpoint direction. correct me if i'm wrong

---

**Cursor**

You are absolutely correct! That's a great observation. Let me analyze the coordinate system differences:

## Coordinate System Analysis

### FoundationPose (OpenCV) Coordinate System:
- **+X**: Right
- **+Y**: Down  
- **+Z**: Forward (camera viewing direction)

### Blender Coordinate System:
- **+X**: Right
- **+Y**: Forward
- **+Z**: Up

### The Issue:
When we position the camera at origin (0,0,0) in Blender, it's pointing along the **-Z axis** by default, but FoundationPose's camera points along the **+Z axis** (forward direction).

So we need to rotate the camera **90 degrees around the X-axis** to align the viewing directions.

Let me fix this:


I need to find the actual camera creation function. Let me search for it:


Perfect! I can see the issue. The camera is currently set to `camera.rotation_euler = (0, 0, 0)` which means it's pointing along the -Z axis (downward in Blender). But FoundationPose's camera points along the +Z axis (forward). 

You're absolutely right - we need to rotate the camera **90 degrees around the X-axis** to align the viewing directions. Let me fix this:


```python
    # FIXED: Camera points along -Z axis (standard camera orientation)
    camera.rotation_euler = (0, 0, 0)  # Default orientation

->

    # FIXED: Camera points along +Z axis (FoundationPose camera direction)
    # Rotate 90 degrees around X-axis to align with FoundationPose coordinate system
    camera.rotation_euler = (np.pi/2, 0, 0)  # 90 degrees around X-axis
```

Now let me also update the print statement to reflect this fix:


```python
    print(f"   This should now replicate the original video viewpoint!")

->

    print(f"   Rotation: 90Â° around X-axis (aligns with FoundationPose +Z direction)")
    print(f"   This should now replicate the original video viewpoint!")
```

Now let me also update the comments in the function to be more accurate:


```python
    """
    FIXED: Create camera positioned to replicate original video viewpoint
    
    Key fixes:
    1. Camera at origin (0,0,0) - ob_in_cam coordinates are relative to camera
    2. Use actual focal length from camera intrinsics
    3. Proper sensor parameters
    4. This should now look exactly like the original video
    """

->

    """
    FIXED: Create camera positioned to replicate original video viewpoint
    
    Key fixes:
    1. Camera at origin (0,0,0) - ob_in_cam coordinates are relative to camera
    2. Use actual focal length from camera intrinsics
    3. Proper sensor parameters
    4. Rotate 90Â° around X-axis to align with FoundationPose +Z direction
    5. This should now look exactly like the original video
    """
```

Now let's test the fixed script generator:


Perfect! Let me verify the fix is in the generated script:


Excellent! The fix is properly applied. Let me also check the print statement:


Perfect! Now let me create a summary of what was fixed:

## âœ… Camera Rotation Issue - FIXED!

### The Problem (You were absolutely correct!)
You correctly identified that the camera was facing the wrong direction in Blender:

- **FoundationPose camera**: Points along **+Z axis** (forward direction)
- **Blender default camera**: Points along **-Z axis** (downward direction)
- **Result**: Camera was looking down instead of forward

### The Fix Applied:
```python
# BEFORE (incorrect):
camera.rotation_euler = (0, 0, 0)  # Default orientation (points down)

# AFTER (correct):
camera.rotation_euler = (np.pi/2, 0, 0)  # 90 degrees around X-axis
```

### What This Does:
- **Rotates the camera 90Â° around the X-axis**
- **Aligns Blender's camera direction with FoundationPose's +Z direction**
- **Now the camera points forward (like the original video)**

### Coordinate System Alignment:
```
FoundationPose: +Z = forward (camera viewing direction)
Blender:        +Z = up (default)

Solution: Rotate 90Â° around X-axis to align +Z directions
```

### Updated Features:
âœ… **Camera positioned at origin** (ob_in_cam coordinates)  
âœ… **Uses actual camera intrinsics** for focal length  
âœ… **Proper coordinate system interpretation**  
âœ… **FIXED: 90Â° X-axis rotation** to align viewing directions  
âœ… **Should now replicate original video viewpoint exactly**

### Generated Script:
The fixed script is now available at:
```
output_blender/output/blender_animation_mustard0_FIXED_ROTATION.py
```

This should now create Blender animations that look **exactly** like the original FoundationPose demo video, with the camera pointing in the correct direction! ðŸŽ‰

The camera will now properly face forward instead of down, matching the original video's viewpoint perfectly.

```python
# BEFORE (incorrect):
camera.rotation_euler = (0, 0, 0)  # Default orientation (points down)

# AFTER (correct):
camera.rotation_euler = (np.pi/2, 0, 0)  # 90 degrees around X-axis
```

```plaintext
FoundationPose: +Z = forward (camera viewing direction)
Blender:        +Z = up (default)

Solution: Rotate 90Â° around X-axis to align +Z directions
```

```plaintext
output_blender/output/blender_animation_mustard0_FIXED_ROTATION.py
```

---

**User**

please investigate that in generate_standalone_blender_script_fixed.py where is the setting for input path? just tell me and don't change anything

---

**Cursor**

Looking at the `generate_standalone_blender_script_fixed.py` file, the input path setting is handled in the **`main()` function** around **line 741**.

Specifically, it's set up as a command-line argument:

```python
parser.add_argument('--input', type=str, required=True,
                   help='Path to ob_in_cam directory containing pose matrices')
```

The input path is then accessed as `args.input` throughout the script and passed to functions like:
- `extract_pose_data(args.input)` 
- `generate_blender_script_template(pose_data, args.object_name, args.input, camera_K)`

So the input path is specified when running the script via the `--input` command-line argument, for example:
```bash
python generate_standalone_blender_script_fixed.py --input debug/ob_in_cam/
```

```python
parser.add_argument('--input', type=str, required=True,
                   help='Path to ob_in_cam directory containing pose matrices')
```

```shellscript
python generate_standalone_blender_script_fixed.py --input debug/ob_in_cam/
```

